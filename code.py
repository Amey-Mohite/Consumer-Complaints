# -*- coding: utf-8 -*-
"""Code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tj11gPaSv3kX5HxZ0E71CNEX0C5mEMgS

###**DATA  and library import and basic Preprocessing**
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install stop_words

import pandas as pd
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.corpus import wordnet as wn
from stop_words import get_stop_words
from string import punctuation
import re, sys, math, string
import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
from sklearn.feature_extraction.text import TfidfVectorizer
from numpy import array
from sklearn.model_selection import train_test_split as tts
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report
from sklearn.ensemble import RandomForestClassifier
from keras.layers import Embedding
from keras.layers import Dense, Input, GlobalMaxPooling1D
from keras.layers import Conv1D, MaxPooling1D, Embedding
from keras.models import Model
from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout,concatenate
from keras.layers.core import Reshape, Flatten
from keras.callbacks import EarlyStopping
from keras.optimizers import Adam
from keras.models import Model
from keras import regularizers
from keras.layers import Embedding
from keras.preprocessing.text import Tokenizer
from keras.wrappers.scikit_learn import KerasClassifier
from ast import literal_eval
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, confusion_matrix
import numpy as np
import itertools 
from keras.models import Sequential
from keras.layers import Dense, LSTM, Embedding
nltk.download('stopwords')
from keras.preprocessing.sequence import pad_sequences
import warnings
warnings.filterwarnings("ignore")
from keras.callbacks import EarlyStopping
from keras.models import load_model
from keras.utils import to_categorical
import logging
from gensim.models import word2vec
import gensim
from sklearn.utils import shuffle
from sklearn.svm import LinearSVC

df = pd.read_csv("/content/drive/My Drive/Consumer Complaints/Consumer_Complaints.csv")

df.head()

df.shape

data = df[["Product", "Consumer complaint narrative"]]

data.head()

data.shape

data["Product"] = data["Product"].str.lower()

"""###**Preprocessing**"""

data.isnull().sum()

data = data.dropna()
data.isnull().sum()

data.head()

data.shape

data["Product"].value_counts()

data.reset_index(level=0, inplace=True)

data = data.drop(['index'],1)

data.head()

data = data.replace(to_replace =["credit card", "prepaid card"],  
                            value ="credit card or prepaid card")
data = data.replace(to_replace ="credit reporting",
                            value ="credit reporting, credit repair services, or other personal consumer reports")
data = data.replace(to_replace ="virtual currency",
                            value ="money transfer, virtual currency, or money service")
data = data.replace(to_replace ="money transfers",
                            value ="money transfer, virtual currency, or money service")
data = data.replace(to_replace =["student loan", "consumer loan", "vehicle loan or lease", "payday loan, title loan, or personal loan", "payday loan"],  
                            value ="loans")
data = data.replace(to_replace ="checking or savings account",  
                            value ="bank account or service")

data = data[data.Product != 'money transfer, virtual currency, or money service']
data = data[data.Product != 'other financial service']

data["Product"].value_counts()

data.head()

stopwordlist = ['title', 'body', 'xxxx']
english_stopwords = get_stop_words(language='english')
english_stopwords += stopwordlist
english_stopwords = list(set(english_stopwords))+list(punctuation)

def get_wordnet_pos(word):
    if word.startswith('N'):
        return wn.NOUN
    elif word.startswith('V'):
        return wn.VERB
    elif word.startswith('J'):
        return wn.ADJ
    elif word.startswith('R'):
        return wn.ADV
    else:
        return wn.NOUN

def tfidf_preprocess(text):
    lemmatizer = nltk.WordNetLemmatizer().lemmatize
    text = re.sub('\W+', ' ', str(text))
    text = re.sub(r'[0-9]+', '', text.lower())
    word_pos = nltk.pos_tag(nltk.word_tokenize(text))
    normalized_text_lst = [lemmatizer(x[0], get_wordnet_pos(x[1])).lower() for x in word_pos]
    stop_words_free = [i for i in normalized_text_lst if i not in english_stopwords and len(i) > 3]
    stop_words_free = list(set(stop_words_free))
    return(stop_words_free)

data['Consumer complaint narrative'] = data['Consumer complaint narrative'].apply(tfidf_preprocess)

data.to_csv("/content/drive/My Drive/Consumer Complaints/output_final_data.csv", index=False)

data = pd.read_csv('/content/drive/My Drive/Consumer Complaints/output_final_data.csv',converters={"Consumer complaint narrative": literal_eval})

data['Product'].unique()

data.head()

data.shape

data = data[data.astype(str)['Consumer complaint narrative'] != '[]']

data.shape

"""###T**FIDF Vectorizer**"""

data_tfidf = data.copy()

min_word_count = 10

le = LabelEncoder()
data_tfidf["Product"] = le.fit_transform(data_tfidf["Product"])

data_tfidf.head()

data_tfidf['Consumer complaint narrative'] = data_tfidf['Consumer complaint narrative'].apply(lambda x: ' '.join(x))

tfidf = TfidfVectorizer(max_features=1500, sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1',stop_words='english')

vector= tfidf.fit_transform(data_tfidf["Consumer complaint narrative"])

X = vector.toarray()

y = data_tfidf["Product"]

X_train,X_test,y_train,y_test=tts(X,y,test_size=0.3,random_state=42)

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    #print(cm)

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()

"""###**Multinomial NB**"""

def Model_MultinomialNB(X_train,y_train,X_test,y_test):
    mnb = MultinomialNB()
    mnb.fit(X_train,y_train)
    print("Test Accuracy for Multinomial NB:",mnb.score(X_test, y_test))
    
    #Confusion Matrix  
    y_pred=mnb.predict(X_test)
    cnf_matrix = confusion_matrix(y_test, y_pred)
    np.set_printoptions(precision=2)
    plt.figure()
    target_names =['bank account or service', 'credit card or prepaid card', 'credit reporting, credit repair services, or other personal consumer reports','debt collection','Loans','mortgage']
    plot_confusion_matrix(cnf_matrix, classes=target_names,
                      title='Confusion matrix, without normalization')
    plt.show()

    #Classification report
    print(classification_report(y_test,y_pred, target_names=target_names))

"""###**Support vector Machine**"""

def Model_SVC(X_train,y_train,X_test,y_test):
    svc_model = LinearSVC()
    svc_model.fit(X_train,y_train)
    print("Test Accuracy for Support Vector Machine:",svc_model.score(X_test, y_test))
    
    #Confusion Matrix  
    y_pred=svc_model.predict(X_test)
    cnf_matrix = confusion_matrix(y_test, y_pred)
    np.set_printoptions(precision=2)
    plt.figure()
    target_names =['bank account or service', 'credit card or prepaid card', 'credit reporting, credit repair services, or other personal consumer reports','debt collection','Loans','mortgage']
    plot_confusion_matrix(cnf_matrix, classes=target_names,
                      title='Confusion matrix, without normalization')
    plt.show()

    #Classification report
    print(classification_report(y_test,y_pred, target_names=target_names))

"""###**Random Forest**"""

def Model_RandomForest(X_train,y_train,X_test,y_test):
    rf = RandomForestClassifier(n_estimators = 100, max_features='sqrt', verbose=1,random_state=42,n_jobs =-1)
    rf.fit(X_train,y_train)
    print("Test Accuracy for Random Forest:",rf.score(X_test, y_test))
    
    #Confusion Matrix  
    y_pred=rf.predict(X_test)
    cnf_matrix = confusion_matrix(y_test, y_pred)
    np.set_printoptions(precision=2)
    plt.figure()
    target_names =['bank account or service', 'credit card or prepaid card', 'credit reporting, credit repair services, or other personal consumer reports','debt collection','Loans','mortgage']
    plot_confusion_matrix(cnf_matrix, classes=target_names,
                      title='Confusion matrix, without normalization')
    plt.show()

    #Classification report
    print(classification_report(y_test,y_pred, target_names=target_names))

"""###**Random Forest, Multinomial NB and support vector machine with TFIDF**"""

Model_MultinomialNB(X_train,y_train,X_test,y_test)

Model_RandomForest(X_train,y_train,X_test,y_test)

Model_SVC(X_train,y_train,X_test,y_test)

"""###**Word2Vec**"""

def get_average_word2vec(complaints_lst, model, num_features=300):
    index2word_set = set(model.wv.index2word)
    vectorized_lst = []
    vectorized_lst = [model[word] if word in index2word_set else np.zeros(num_features) for word in \
                      complaints_lst]    
    nwords = len(vectorized_lst)
    summed = np.sum(vectorized_lst, axis=0)
    averaged_vector = np.divide(summed, nwords)
    return averaged_vector

data_word2vec = data.copy()
vocabulary_of_all_words = data_word2vec['Consumer complaint narrative'].tolist()
num_features = 300
min_word_count = 10                      
num_workers = 8
context = 10          # Context window size                                                                                    
downsampling = 1e-3   # Downsampling for frequent words
word2vec_model_name = "/content/drive/My Drive/Consumer Complaints/300features_10minwords_10context"
word2vec_complaints = word2vec.Word2Vec(vocabulary_of_all_words, workers=num_workers, size=num_features, 
                                   min_count=min_word_count, window=context, sample=downsampling)
word2vec_complaints.save(word2vec_model_name)

word2vec_complaints = gensim.models.Word2Vec.load(word2vec_model_name)

vocab_lst_flat = [item for sublist in vocabulary_of_all_words for item in sublist]
vocab_lst_flat = list(set(vocab_lst_flat))

try:
    word2vec_complaints.wv.most_similar("good")
except KeyError:
    print("Sorry, this word doesn't exist in the vocabulary.")
    
words_not_present = 0
words_present = 0
total_unique_tokens = len(set(vocab_lst_flat))
missing_words = []
for i in vocab_lst_flat:
    try:
        p = word2vec_complaints[i]
        words_present+=1
    except KeyError:
        missing_words.append(i)
        words_not_present+=1
print(words_present, words_not_present, total_unique_tokens)

print(missing_words[:20])

embeddings_df = data_word2vec['Consumer complaint narrative'].apply(lambda complaint: get_average_word2vec(complaint, word2vec_complaints, num_features)).to_frame()

embeddings_df.head()

col_lst = []
for i in range(num_features):
    col_lst.append('vec_'+str(i+1))

exploded_em_df = pd.DataFrame(embeddings_df['Consumer complaint narrative'].tolist(), columns=col_lst)

exploded_em_df = pd.DataFrame(embeddings_df['Consumer complaint narrative'].apply(pd.Series))

exploded_em_df.head()

exploded_em_df.to_csv("/content/drive/My Drive/Consumer Complaints/vocab_trained_word2Vec.csv", index=False)

exploded_em_df = pd.read_csv('/content/drive/My Drive/Consumer Complaints/vocab_trained_word2Vec.csv')
print("Word2Vec output:\n")
exploded_em_df.head()

data_word2vec = data_word2vec.reset_index(drop=True)
vectorized_df = pd.concat([exploded_em_df, data_word2vec[['Product']]], axis=1)                        
vectorized_df = shuffle(vectorized_df)

if vectorized_df[vectorized_df.isnull().any(axis=1)].empty:
    res = "True" # No NaNs exist in the cleaned dataset.
else:
    res = "False"
print(res)
print(vectorized_df.shape)
if not res:
    vectorized_df[vectorized_df.isnull().any(axis=1)]
    vectorized_df.dropna(axis=0, how='any')
    print(vectorized_df.shape)

vectorized_data = np.array(vectorized_df.drop('Product', axis=1))
vectorized_target = np.array(vectorized_df['Product'])

X_train_word2vec, X_test_word2vec, y_train_word2vec, y_test_word2vec = tts(vectorized_data, vectorized_target,
                                                    test_size=0.3,
                                                    random_state=42)

"""###**Random Forest & Support Vector Machine ![alt text](https://)with Word2Vec**"""

Model_RandomForest(X_train_word2vec, y_train_word2vec, X_test_word2vec, y_test_word2vec)

Model_SVC(X_train_word2vec, y_train_word2vec, X_test_word2vec, y_test_word2vec)

"""###**Convolution Neural Network**"""

X_train_nn, X_test_nn, y_train_nn, y_test_nn = tts(data_tfidf["Consumer complaint narrative"], data_tfidf["Product"], test_size=0.3,random_state=42)

vectorizer = TfidfVectorizer(max_features=1500, sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1',stop_words='english')
X_train_onehot = vectorizer.fit_transform(X_train_nn)

word2idx = {word: idx for idx, word in enumerate(vectorizer.get_feature_names())}
tokenize = vectorizer.build_tokenizer()
preprocess = vectorizer.build_preprocessor()

def to_sequence(tokenizer, preprocessor, index, text):
    words = tokenizer(preprocessor(text))
    indexes = [index[word] for word in words if word in index]
    return indexes

X_train_sequences = [to_sequence(tokenize, preprocess, word2idx, x) for x in X_train_nn]

MAX_SEQ_LENGHT = len(max(X_train_sequences, key=len))
print("MAX_SEQ_LENGHT=", MAX_SEQ_LENGHT)

N_FEATURES = len(vectorizer.get_feature_names())
X_train_sequences = pad_sequences(X_train_sequences, maxlen=MAX_SEQ_LENGHT, value=N_FEATURES)

X_test_sequences = [to_sequence(tokenize, preprocess, word2idx, x) for x in X_test_nn]
X_test_sequences = pad_sequences(X_test_sequences, maxlen=MAX_SEQ_LENGHT, value=N_FEATURES)

y_train_nn1 = pd.get_dummies(y_train_nn)

early_stopping_monitor = EarlyStopping(patience=5)

model_CNN = Sequential()
model_CNN.add(Embedding(len(vectorizer.get_feature_names()) + 1,
                    64,
                    input_length=MAX_SEQ_LENGHT))
model_CNN.add(Conv1D(64, 5, activation='relu'))
model_CNN.add(MaxPooling1D(5))
model_CNN.add(Flatten())
model_CNN.add(Dense(units=64, activation='relu'))
model_CNN.add(Dense(units=6, activation='softmax'))
 
model_CNN.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model_CNN.summary())

History = model_CNN.fit(X_train_sequences[:217176], y_train_nn1[:217176], 
          epochs=30, batch_size=512,
          validation_data=(X_train_sequences[217176:], y_train_nn1[217176:]),callbacks=[early_stopping_monitor], verbose=1)

model_CNN.save("/content/drive/My Drive/Consumer Complaints/model_CNN.h5")

y_test_nn1 = pd.get_dummies(y_test_nn)

model_CNN_loaded = load_model('/content/drive/My Drive/Consumer Complaints/model_CNN.h5')

scores = model_CNN_loaded.evaluate(X_test_sequences, y_test_nn1, verbose=1)
print("CNN - Accuracy:", scores[1])

print("Confusion matrix for CNN")
y_pred = model_CNN_loaded.predict_classes(X_test_sequences)
cnf_matrix = confusion_matrix(y_test_nn, y_pred)
np.set_printoptions(precision=2)
plt.figure()
target_names =['bank account or service', 'credit card or prepaid card', 'credit reporting, credit repair services, or other personal consumer reports','debt collection','Loans','mortgage']
plot_confusion_matrix(cnf_matrix, classes=target_names,
                  title='Confusion matrix, without normalization')
plt.show()

print("Classification Report for CNN")
print(classification_report(y_test_nn,y_pred, target_names=target_names))

"""###**LSTM**"""

model_LSTM = Sequential()
model_LSTM.add(Embedding(len(vectorizer.get_feature_names()) + 1,
                    64,
                    input_length=MAX_SEQ_LENGHT))
model_LSTM.add(LSTM(64))
model_LSTM.add(Dense(units=6, activation='softmax'))
 
model_LSTM.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model_LSTM.summary())

model_LSTM.fit(X_train_sequences[:217176], y_train_nn1[:217176], 
          epochs=30, batch_size=512,
          validation_data=(X_train_sequences[217176:], y_train_nn1[217176:]),callbacks=[early_stopping_monitor], verbose=1)

model_LSTM.save("/content/drive/My Drive/Consumer Complaints/model_LSTM.h5")

scores = model_LSTM.evaluate(X_test_sequences, y_test, verbose=1)
print("LSTM-Accuracy:", scores[1])

print("Confusion matrix for LSTM")
y_pred = model_CNN_loaded.predict_classes(X_test_sequences)
cnf_matrix = confusion_matrix(y_test_nn, y_pred)
np.set_printoptions(precision=2)
plt.figure()
target_names =['bank account or service', 'credit card or prepaid card', 'credit reporting, credit repair services, or other personal consumer reports','debt collection','Loans','mortgage']
plot_confusion_matrix(cnf_matrix, classes=target_names,
                  title='Confusion matrix, without normalization')
plt.show()

print("Classification Report for LSTM")
print(classification_report(y_test_nn,y_pred, target_names=target_names))

